{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Hive schema for Parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to create a hive table for existing Parquet files. <br>\n",
    "This can be done for a single file as well as for multiple files residing under the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating of spark context with hive support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Import parquet schema to hive\").config(\"hive.metastore.uris\", \"thrift://trino-hive:9083\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function below for getting sql script needed for creating table in hive using dataframe types as columns to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCreateTableScript(databaseName, tableName, path, df, partitions=[]):\n",
    "    #remove partition columns from the df to avoid repetition exception\n",
    "    partition_names = map(lambda x: x.split(' ')[0] , partitions )\n",
    "    ndf = df.drop(*partition_names)\n",
    "        \n",
    "    cols = ndf.dtypes\n",
    "    createScript = \"CREATE EXTERNAL TABLE \" + databaseName + \".\" + tableName + \"(\"\n",
    "    colArray = []\n",
    "    for colName, colType in cols:\n",
    "        colArray.append(colName.replace(\" \", \"_\") + \" \" + colType)\n",
    "    createColsScript =   \", \".join(colArray ) + \") \"\n",
    "    partitionBy = \"\"\n",
    "    if len(partitions) > 0:\n",
    "        partitionBy = \"PARTITIONED BY (\" + \", \".join(partitions) + \") \"\n",
    "    script = createScript + createColsScript + partitionBy + \" STORED AS PARQUET LOCATION '\" + path + \"'\"\n",
    "    print(script)\n",
    "    return script\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define main function for creating table where arqument 'path' is path to parquet files \n",
    "def createTable(databaseName, tableName, path, partitions=[]): \n",
    "    df = spark.read.parquet(path)\n",
    "    sqlScript = getCreateTableScript(databaseName, tableName, path, df, partitions)\n",
    "    spark.sql(sqlScript)\n",
    "    if len(partitions) > 0:\n",
    "        spark.sql(f'msck repair table {databaseName}.{tableName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One file example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE EXTERNAL TABLE default.tab1_single_file(registration_dttm timestamp, id int, first_name string, last_name string, email string, gender string, ip_address string, cc string, country string, birthdate string, salary double, title string, comments string)  STORED AS PARQUET LOCATION 'v3io://users/dani/examples/userdata1.parquet'\n"
     ]
    }
   ],
   "source": [
    "# Set the path where the parquet file is located.\n",
    "my_parqute_file_path = os.path.join('v3io://users/'+os.getenv('V3IO_USERNAME')+'/examples/userdata1.parquet')\n",
    "\n",
    "createTable(\"default\",\"tab1_single_file\",my_parqute_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variable $DATABASE_URL not set, and no connect string given.\n",
      "Connection info needed in SQLAlchemy format, example:\n",
      "               postgresql://username:password@hostname/dbname\n",
      "               or an existing connection: dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "%sql select * from hive.default.tab1_single_file limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One folder example for spark output job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE EXTERNAL TABLE default.table_from_dir(registration_dttm timestamp, id int, first_name string, last_name string, email string, gender string, ip_address string, cc string, country string, birthdate string, salary double, title string, comments string)  STORED AS PARQUET LOCATION 'v3io://users/dani/examples/spark-output/'\n"
     ]
    }
   ],
   "source": [
    "# Set the path where the parquet folder is located.\n",
    "folder_path = os.path.join('v3io://users/'+os.getenv('V3IO_USERNAME')+'/examples/spark-output/')\n",
    "\n",
    "createTable(\"default\",\"table_from_dir\",folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variable $DATABASE_URL not set, and no connect string given.\n",
      "Connection info needed in SQLAlchemy format, example:\n",
      "               postgresql://username:password@hostname/dbname\n",
      "               or an existing connection: dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "%sql select * from hive.default.table_from_dir limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioned parquet example\n",
    "\n",
    "Table partitioning is a common optimization approach used in systems like Hive. In a partitioned table, data are usually stored in different directories, with partitioning column values encoded in the path of each partition directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE EXTERNAL TABLE default.partitioned_table(registration_dttm timestamp, id int, first_name string, last_name string, email string, ip_address string, cc string, country string, birthdate string, salary double, title string, comments string) PARTITIONED BY (gender string)  STORED AS PARQUET LOCATION 'v3io://users/dani/examples/partitioned_pq'\n"
     ]
    }
   ],
   "source": [
    "# Set path where parquet folder with parquet partitions are located indside.\n",
    "folder_path = os.path.join('v3io://users/'+os.getenv('V3IO_USERNAME')+'/examples/partitioned_pq')\n",
    "#provide list of partitions and their type\n",
    "partition_list = [\"gender string\"]                       \n",
    "createTable(\"default\", \"partitioned_table\", folder_path, partition_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variable $DATABASE_URL not set, and no connect string given.\n",
      "Connection info needed in SQLAlchemy format, example:\n",
      "               postgresql://username:password@hostname/dbname\n",
      "               or an existing connection: dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "%sql select * from hive.default.partitioned_table limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding new partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Once added new partitions to the table, \n",
    "# it is required to run the below command in order for the hive metastore to be aware of the new files.\n",
    "spark.sql('msck repair table default.partitioned_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Browse the Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+-----------------+-----------+\n",
      "|namespace|        tableName|isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "|  default|        csv_table|      false|\n",
      "|  default|            demo1|      false|\n",
      "|  default|            demo2|      false|\n",
      "|  default|         example2|      false|\n",
      "|  default|partitioned_table|      false|\n",
      "|  default| tab1_single_file|      false|\n",
      "|  default|   table_from_dir|      false|\n",
      "|  default|  table_from_dir2|      false|\n",
      "+---------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test how the tables were saved\n",
    "#spark.sql(\"drop database test CASCADE\")\n",
    "databaseName = \"default\"\n",
    "\n",
    "spark.sql(\"show databases\").show()\n",
    "spark.sql(\"show tables in \" + databaseName).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Hive from command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run Hive from command line,open up a jupyter terminal and run \"hive\" <br>\n",
    "To view all existing hive tables run: show tables; <br>\n",
    "Here you can run queries without specifying Hive. <br>\n",
    "e.g. select * from table_from_single_file2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "This will only clean the metastore definitions.\n",
    "<br>The underlying data won't be affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table \" + databaseName + \".tab1_single_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table \" + databaseName + \".table_from_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table \" + databaseName + \".partitioned_table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
